{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:22:01.341707Z",
     "start_time": "2019-10-14T23:22:01.271092Z"
    }
   },
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "#regex imports\n",
    "import re\n",
    "import string\n",
    "\n",
    "#graph imports\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "#scraping imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import os\n",
    "\n",
    "#NLP imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "#Cluster analysis imports\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get URLs for XML files on Bundestag website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:22:06.518481Z",
     "start_time": "2019-10-14T23:22:01.350500Z"
    }
   },
   "outputs": [],
   "source": [
    "# run Chrome with Selenium\n",
    "chromedriver = \"/Applications/chromedriver\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "url = \"https://www.bundestag.de/services/opendata\"\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:22:06.575743Z",
     "start_time": "2019-10-14T23:22:06.524368Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_to_link_list ():\n",
    "    '''finds all XML links and adds them to a list'''\n",
    "    soup_links = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    regex = re.compile('.*XML .*')\n",
    "    documents = soup_links.find_all(title=regex)\n",
    "    \n",
    "    for i in documents:\n",
    "        link_list.append(i['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:22:51.607982Z",
     "start_time": "2019-10-14T23:22:06.602252Z"
    }
   },
   "outputs": [],
   "source": [
    "#click through all pages on Bundestag website with parliament transcripts for current term of office \n",
    "link_list = []\n",
    "\n",
    "for i in range(21):\n",
    "    button = driver.find_element_by_xpath(\"//button[@type='button'][@class='slick-next slick-arrow']\")\n",
    "    button.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "add_to_link_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:22:51.613801Z",
     "start_time": "2019-10-14T23:22:51.610430Z"
    }
   },
   "outputs": [],
   "source": [
    "#create list with complete URLs\n",
    "\n",
    "url_list = []\n",
    "url_start = 'https://www.bundestag.de'\n",
    "\n",
    "for i in link_list:\n",
    "    url_list.append(url_start + i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Scraping and parsing of XML files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:22:51.651435Z",
     "start_time": "2019-10-14T23:22:51.616280Z"
    }
   },
   "outputs": [],
   "source": [
    "# create empty dictionary for data scraping\n",
    "keys = ['speech_ID','date','speaker_ID','first_name','last_name','party','speech','comments']\n",
    "data_dict = {}\n",
    "\n",
    "for i in keys:\n",
    "    data_dict[i] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:23:59.133633Z",
     "start_time": "2019-10-14T23:22:51.655225Z"
    }
   },
   "outputs": [],
   "source": [
    "#run scraping and parsing\n",
    "\n",
    "fail_list = []\n",
    "\n",
    "for url_ in url_list:\n",
    "    try:\n",
    "        url = url_\n",
    "        user_agent = {'User-Agent' : 'Mozilla/5.0'}\n",
    "\n",
    "        response = requests.get(url, headers=user_agent)\n",
    "        \n",
    "        page = response.text\n",
    "        soup = BeautifulSoup(page, \"lxml\")\n",
    "        \n",
    "        speeches = soup.find_all('rede')\n",
    "        \n",
    "        for i in speeches:\n",
    "    \n",
    "            #ID speech\n",
    "            speech_ID = i['id']\n",
    "        \n",
    "            #date\n",
    "            date = soup.find('datum').attrs['date']\n",
    "\n",
    "            #ID speaker\n",
    "            speaker_ID = i.find(klasse='redner').find('redner').attrs['id']\n",
    "\n",
    "            #first name\n",
    "            first_name = i.find(klasse='redner').find('redner').find('vorname').text\n",
    "\n",
    "            #last name\n",
    "            last_name = i.find(klasse='redner').find('redner').find('nachname').text\n",
    "\n",
    "            #party/ role\n",
    "            try:\n",
    "                party = i.find(klasse='redner').find('redner').find('rolle_lang').text\n",
    "            except:\n",
    "                party = i.find(klasse='redner').find('redner').find('fraktion').text\n",
    "\n",
    "            #speech, exclude first item in list as it is ID, name etc.\n",
    "            speech_list = i.find_all('p')[1:]\n",
    "            speech = ''\n",
    "            for j in speech_list:\n",
    "                speech += j.text\n",
    "\n",
    "            #comments\n",
    "            comments = []\n",
    "            comments_list = i.find_all('kommentar')\n",
    "            for i in comments_list:\n",
    "                comments.append(i.text)\n",
    "\n",
    "            #populate dictionary\n",
    "            for i in keys:\n",
    "                data_dict[i].append(eval(i))\n",
    "    except:\n",
    "        fail_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:23:59.178142Z",
     "start_time": "2019-10-14T23:23:59.137448Z"
    }
   },
   "outputs": [],
   "source": [
    "#create dataframe\n",
    "df = pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T03:09:36.264809Z",
     "start_time": "2019-09-24T03:09:36.261018Z"
    }
   },
   "source": [
    "# 4. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Clean information on parliament member"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T03:44:50.177767Z",
     "start_time": "2019-09-24T03:44:50.174576Z"
    }
   },
   "source": [
    "### 4.1.a Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:23:59.193661Z",
     "start_time": "2019-10-14T23:23:59.182234Z"
    }
   },
   "outputs": [],
   "source": [
    "#add full name column\n",
    "df['full_name'] = df['last_name'] + ', ' + df['first_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:23:59.211336Z",
     "start_time": "2019-10-14T23:23:59.198336Z"
    }
   },
   "outputs": [],
   "source": [
    "#lists with parliament roles\n",
    "\n",
    "CDU_secretaries = (['Merkel, Angela','Scheuer, Andreas','Klöckner, Julia','Altmaier, Peter','Karliczek, Anja',\n",
    "                    'Spahn, Jens','Seehofer, Horst','Müller, Gerd','Leyen, Ursula','Kramp-Karrenbauer, Annegret'])\n",
    "\n",
    "SPD_secretaries = (['Schulze, Svenja','Heil, Hubertus','Giffey, Franziska','Barley, Katarina','Lambrecht, Christine',\n",
    "                    'Scholz, Olaf','Maas, Heiko'])\n",
    "\n",
    "\n",
    "CDU_undersecretaries = (['Barthle, Norbert','Bär, Dorothee','Braun, Helge', 'Bareiß, Thomas','Bilger, Steffen',\n",
    "                         'Ferlemann, Enak','Flachsbarth, Maria','Fuchtel, Hans-Joachim', 'Gebhart, Thomas',\n",
    "                         'Grütters, Monika','Hirte, Christian','Hoppenstedt, Hendrik','Krings, Günter',\n",
    "                         'Mayer, Stephan','Meister, Michael','Rachel, Thomas','Stübgen, Michael','Tauber, Peter',\n",
    "                         'Wanderwitz, Marco','Weiss, Sabine','Widmann-Mauz, Annette','Wittke, Oliver'])\n",
    "\n",
    "SPD_undersecretaries = (['Annen, Niels','Hagedorn, Bettina','Hagl-Kehl, Rita','Griese, Kerstin','Kramme, Anette',\n",
    "                         'Lange, Christian','Lambrecht, Christine','Marks, Caren','Müntefering, Michelle',\n",
    "                         'Pronold, Florian','Roth, Michael','Ryglewski, Sarah','Schwarzelühr-Sutter, Rita',\n",
    "                         'Silberhorn, Thomas'])\n",
    "\n",
    "CDU_other_roles = ['Brauksiepe, Ralf','Spahn, Jens','Schäuble, Dr. Wolfgang','Karliczek, Anja']\n",
    "\n",
    "independent_members = (['Bülow, Marco', 'Mieruch, Mario', 'Petry, Frauke','Kamann, Uwe'])\n",
    "\n",
    "secretary_list = CDU_secretaries + SPD_secretaries\n",
    "\n",
    "undersecretary_list = CDU_undersecretaries + SPD_undersecretaries\n",
    "\n",
    "cabinet_list = secretary_list + undersecretary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:23:59.220860Z",
     "start_time": "2019-10-14T23:23:59.215368Z"
    }
   },
   "outputs": [],
   "source": [
    "#create party dictionary\n",
    "party_dict = {'CDU/CSU': 'CDU/CSU','AfD': 'AfD', 'SPD': 'SPD','FDP': 'FDP','DIE LINKE':'DIE LINKE',\n",
    "               'BÜNDNIS 90/DIE GRÜNEN':'BÜNDNIS 90/DIE GRÜNEN'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:23:59.231782Z",
     "start_time": "2019-10-14T23:23:59.222914Z"
    }
   },
   "outputs": [],
   "source": [
    "#dictionary with resorts\n",
    "\n",
    "resort_dict = ({'Merkel, Angela':'Bundeskanzlerin','Scholz, Olaf':'Finanzen','Seehofer, Horst':'Inneres, Bau und Heimat',\n",
    "                'Maas, Heiko':'Auswärtiges','Altmaier, Peter':'Wirtschaft und Energie','Barley, Katarina':'Justiz und Verbraucherschutz',\n",
    "                'Lambrecht, Christine':'Justiz und Verbraucherschutz/ Finanzen','Heil, Hubertus':'Arbeit und Soziales',\n",
    "                'Leyen, Ursula':'Verteidigung','Kramp-Karrenbauer, Annegret':'Verteidigung','Klöckner, Julia':'Ernährung und Landwirtschaft',\n",
    "                'Giffey, Franziska':'Familie, Senioren, Frauen und Jugend','Spahn, Jens':'Gesundheit',\n",
    "                'Scheuer, Andreas':'Verkehr und digitale Infrastruktur', 'Schulze, Svenja':'Umwelt, Naturschutz und nukleare Sicherheit',\n",
    "                'Karliczek, Anja':'Bildung und Forschung','Müller, Gerd':'Wirtschaftliche Zusammenarbeit und Entwicklung',\n",
    "                'Braun, Helge':'Bundeskanzleramt',\n",
    "                'Grütters, Monika':'Kultur und Medien','Hoppenstedt, Hendrik':'Bürokratieabbau und bessere Rechtsetzung, Bund-Länder-Beziehungen',\n",
    "                'Widmann-Mauz, Annette':'Migration, Flüchtlinge und Integration','Bär, Dorothee':'Digitalisierung',\n",
    "                'Hagedorn, Bettina':'Finanzen','Ryglewski Sarah':'Finanzen','Krings, Günter':'Inneres, Bau und Heimat',\n",
    "                'Wanderwitz, Marco':'Inneres, Bau und Heimat','Mayer, Stephan':'Inneres, Bau und Heimat',\n",
    "                'Annen, Niels':'Auswärtiges','Müntefering, Michelle':'Auswärtiges - Internationale Kulturpolitik',\n",
    "                'Roth, Michael':'Auswärtiges - Europa, Deutsch-französische Zusammenarbeit',\n",
    "                'Bareiß, Thomas':'Wirtschaft und Energie - Tourismus','Hirte, Christian':'Wirtschaft und Energie - Neue Bundesländer',\n",
    "                'Wittke, Oliver':'Wirtschaft und Energie - EITI','Hagl-Kehl, Rita':'Justiz und Verbraucherschutz',\n",
    "                'Lange, Christian':'Justiz und Verbraucherschutz','Griese, Kerstin':'Arbeit und Soziales',\n",
    "                'Kramme, Anette':'Arbeit und Soziales','Tauber, Peter':'Verteidigung','Silberhorn, Thomas':'Verteidigung',\n",
    "                'Fuchtel, Hans-Joachim':'Ernährung und Landwirtschaft','Stübgen, Michael':'Ernährung und Landwirtschaft',\n",
    "                'Marks, Caren':'Familie, Senioren, Frauen und Jugend','Zierke, Stefan':'Familie, Senioren, Frauen und Jugend',\n",
    "                'Gebhart, Thomas':'Gesundheit','Weiss, Sabine':'Gesundheit','Bilger, Steffen':'Verkehr und digitale Infrastruktur',\n",
    "                'Ferlemann, Enak':'Verkehr und digitale Infrastruktur','Pronold, Florian':'Umwelt, Naturschutz und nukleare Sicherheit',\n",
    "                'Schwarzelühr-Sutter, Rita':'Umwelt, Naturschutz und nukleare Sicherheit','Meister, Michael':'Bildung und Forschung',\n",
    "                'Rachel, Thomas':'Bildung und Forschung','Barthle, Norbert':'Wirtschaftliche Zusammenarbeit und Entwicklung',\n",
    "                'Flachsbarth, Maria':'Wirtschaftliche Zusammenarbeit und Entwicklung'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:23:59.237753Z",
     "start_time": "2019-10-14T23:23:59.233860Z"
    }
   },
   "outputs": [],
   "source": [
    "#dictionary with duplicate names\n",
    "duplicates_dict = ({'Bluhm, Heidrun':'Bluhm-Förster, Heidrun','Cotar, Joana Eleonora':'Cotar, Joana',\n",
    "                    'Gauland, Alexander':'Gauland, Eberhardt Alexander',\n",
    "                    'Elsner von Gronow, Berengar':'Gronow, Berengar Elsner von',\n",
    "                    'Kuhle, Konstantin':'Kuhle, Konstantin Elias',\n",
    "                    'Schreiber, Eva-Maria':'Schreiber, Eva-Maria Elisabeth','Weiler, Albert':'Weiler, Albert H.'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.b Create functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:23:59.247331Z",
     "start_time": "2019-10-14T23:23:59.239618Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_party_1(row):\n",
    "    '''adjusts party for parliament members with official positions or without party membership'''\n",
    "    \n",
    "    #CDU/CSU\n",
    "    if row['full_name'] in CDU_secretaries:\n",
    "        return 'CDU/CSU'\n",
    "    elif row['full_name'] in CDU_undersecretaries:\n",
    "        return 'CDU/CSU'\n",
    "    elif row['full_name'] in CDU_other_roles:\n",
    "        return 'CDU/CSU'\n",
    "    \n",
    "    #SPD\n",
    "    elif row['full_name']  in SPD_secretaries:\n",
    "        return 'SPD'    \n",
    "    elif row['full_name']  in SPD_undersecretaries:\n",
    "        return 'SPD'\n",
    "    \n",
    "    #independent\n",
    "    elif row['full_name']  in independent_members:\n",
    "        return 'Fraktionslos'\n",
    "    \n",
    "    #all other\n",
    "    else:\n",
    "        return row['party']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:23:59.259018Z",
     "start_time": "2019-10-14T23:23:59.251063Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_party_2(string):\n",
    "    '''identifies guest speakers who are not parliament members as other'''\n",
    "    try:\n",
    "        party = party_dict[string]\n",
    "    except:\n",
    "        party = 'Other'\n",
    "    return party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:23:59.267833Z",
     "start_time": "2019-10-14T23:23:59.263076Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_cabinet(name):\n",
    "    '''get role in parliament'''\n",
    "    \n",
    "    if name in cabinet_list:\n",
    "        return 'Minister'\n",
    "    elif name in secretary_list:\n",
    "        return 'Staatssekretär/-minister'\n",
    "    else:\n",
    "        return 'Parlament'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:23:59.274535Z",
     "start_time": "2019-10-14T23:23:59.270639Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_resort(name):\n",
    "    '''add resort information'''\n",
    "    \n",
    "    if name in list(resort_dict.keys()):\n",
    "        return resort_dict[name]\n",
    "    else:\n",
    "        return 'n/a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:23:59.279973Z",
     "start_time": "2019-10-14T23:23:59.276545Z"
    }
   },
   "outputs": [],
   "source": [
    "def adjust_duplicates(string):\n",
    "    '''adjusts duplicate names'''\n",
    "    if string in list(duplicates_dict.keys()):\n",
    "        return duplicates_dict[string]\n",
    "    else:\n",
    "        return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T03:49:56.528107Z",
     "start_time": "2019-09-24T03:49:56.524173Z"
    }
   },
   "source": [
    "### 4.1.c Apply functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:23:59.973493Z",
     "start_time": "2019-10-14T23:23:59.282226Z"
    }
   },
   "outputs": [],
   "source": [
    "df['party'] = df.apply(get_party_1, axis=1)\n",
    "df['party'] = df['party'].apply(get_party_2)\n",
    "df['cabinet'] = df['full_name'].apply(get_cabinet)\n",
    "df['resort'] = df['full_name'].apply(get_resort)\n",
    "df['full_name'] = df['full_name'].apply(adjust_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Clean speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:24:10.022186Z",
     "start_time": "2019-10-14T23:23:59.975728Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create baseline \n",
    "df['speech_clean'] = df['speech']\n",
    "\n",
    "#Remove numbers\n",
    "alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "df['speech_clean'] = df['speech_clean'].apply(alphanumeric)\n",
    "\n",
    "#Remove punctuation\n",
    "punctuation = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n",
    "df['speech_clean'] = df['speech_clean'].apply(punctuation)\n",
    "\n",
    "#Remove non-breaking space\n",
    "xa0 = lambda x: re.sub('\\xa0', ' ', x)\n",
    "xad = lambda x: re.sub('\\xad', ' ', x)\n",
    "df['speech_clean'] = df['speech_clean'].apply(xa0).apply(xad)\n",
    "\n",
    "#Remove dash\n",
    "dash = lambda x: re.sub('–', ' ', x)\n",
    "df['speech_clean'] = df['speech_clean'].apply(dash)\n",
    "\n",
    "#Make lower case\n",
    "lower = lambda x: x.lower()\n",
    "df['speech_clean'] = df['speech_clean'].apply(lower)\n",
    "\n",
    "#Replace hyphens\n",
    "hyphen_1 = lambda x: re.sub('“', ' ', x)\n",
    "hyphen_2 = lambda x: re.sub('„', ' ', x)\n",
    "df['speech_clean'] = df['speech_clean'].apply(hyphen_1).apply(hyphen_2)\n",
    "\n",
    "#Remove doubles space\n",
    "spaces = lambda x: ' '.join(x.split())\n",
    "df['speech_clean'] = df['speech_clean'].apply(spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Remove speeches with less than 50 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:24:10.027754Z",
     "start_time": "2019-10-14T23:24:10.024586Z"
    }
   },
   "outputs": [],
   "source": [
    "def word_count(string):\n",
    "    '''counts number of words in speech'''\n",
    "    count = len(string.split())\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:24:10.545501Z",
     "start_time": "2019-10-14T23:24:10.030162Z"
    }
   },
   "outputs": [],
   "source": [
    "df['word_count'] = df['speech_clean'].apply(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:24:10.580379Z",
     "start_time": "2019-10-14T23:24:10.549838Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[df['word_count']>49]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Word tokenization and vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T04:08:28.637011Z",
     "start_time": "2019-09-24T04:08:28.633822Z"
    }
   },
   "source": [
    "## 5.1. Tokenization and vectorization preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:24:10.593150Z",
     "start_time": "2019-10-14T23:24:10.583873Z"
    }
   },
   "outputs": [],
   "source": [
    "#load stopwords\n",
    "stop_words = stopwords.words('german')\n",
    "\n",
    "# add manual stopwords\n",
    "manual_stop_words = (['werd','herr','kolleg','mehr','woll','wer', 'gut', 'wichtig','uber','konn','sag','frag', 'sag',\n",
    "                      'frau', 'herr''schon', 'wurd', 'gibt', 'thema', 'ganz', 'mal', 'konn', 'glaub', 'gesagt', 'mach',\n",
    "                      'geht', 'stell', 'all', 'and', 'red', 'hatt', 'debatt','mocht', 'dank', 'word', 'lieb', 'letzt',\n",
    "                      'find', 'darub', 'darauf', 'desweg','eigent', 'vielleicht', 'genau', 'gar','bundesregier','deutsch'])\n",
    "\n",
    "stop_words = stop_words + manual_stop_words\n",
    "\n",
    "#load stemmer\n",
    "stemmer = SnowballStemmer('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:24:10.600444Z",
     "start_time": "2019-10-14T23:24:10.595320Z"
    }
   },
   "outputs": [],
   "source": [
    "#create tokenizer and stemmer function\n",
    "def tokenize_and_stem(text):\n",
    "    '''applies tokenization and stemming'''\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        token = stemmer.stem(token)\n",
    "        if len(token) > 2:\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T04:08:51.400888Z",
     "start_time": "2019-09-24T04:08:51.396171Z"
    }
   },
   "source": [
    "## 5.2. Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:25:56.921294Z",
     "start_time": "2019-10-14T23:24:10.602740Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['bundesregi', 'dasselb', 'demselb', 'denselb', 'derselb', 'dess', 'desselb', 'dieselb', 'eur', 'fur', 'geg', 'gewes', 'hint', 'ind', 'jed', 'jen', 'konnt', 'manch', 'musst', 'ohn', 'selb', 'solch', 'sollt', 'sond', 'unt', 'wahrend', 'weit', 'welch', 'wied', 'wollt', 'zwisch'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=stop_words,tokenizer=tokenize_and_stem,max_df=0.9)\n",
    "sparse_tfidf = tfidf.fit_transform(df['speech_clean'])\n",
    "tfidf_feature_names = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T04:10:53.805406Z",
     "start_time": "2019-09-24T04:10:53.799757Z"
    }
   },
   "source": [
    "# 6. Topic modeling - NMF (35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T04:11:15.914393Z",
     "start_time": "2019-09-24T04:11:15.911021Z"
    }
   },
   "source": [
    "## 6.1. Topic modeling preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:25:56.932581Z",
     "start_time": "2019-10-14T23:25:56.924310Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    '''shows topics identified in topic modeling'''\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T04:14:30.046741Z",
     "start_time": "2019-09-24T04:14:30.042296Z"
    }
   },
   "source": [
    "## 6.2. Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:27:34.610790Z",
     "start_time": "2019-10-14T23:25:56.936681Z"
    }
   },
   "outputs": [],
   "source": [
    "nmf_35 = NMF(35,random_state=1)\n",
    "nmf_35_topic = nmf_35.fit_transform(sparse_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Topic identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:27:34.621687Z",
     "start_time": "2019-10-14T23:27:34.612880Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_names = (['Füllwörter','Europa/EU-Politik','Bundeswehr','Haushalt','Familienpolitik','Parlamentsabstimmung',\n",
    "                   'Gesetzgebung','Bildungspolitik','Arbeitsmarkt','Miete/Wohnen','Umwelt/Energie','Sudaneinsatz',\n",
    "                   'NATO/Sicherheitspolitik','Pflegepolitik','Studium','Parlamentsdebatten','Organspende',\n",
    "                   'Automobilverkehr','Ländlicher Raum','Wirtschaft','Förderalismus','Grundgesetz/Demokratie',\n",
    "                   'Syrienkonflikt/Naher Osten','Kosovoeinsatz','Nahostkonflikt','Innenpolitik/Rechtstaat',\n",
    "                   'Flüchtlingspolitik','Seenotrettung','Gesundheitspolitik','Afghanistaneinsatz','Brexit',\n",
    "                   'Digitaliserung','Finanzmarktpolitik','Malieinsatz','Landwirtschaft'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:27:35.160151Z",
     "start_time": "2019-10-14T23:27:34.849843Z"
    }
   },
   "outputs": [],
   "source": [
    "#create dataframe with topic values\n",
    "H_35 = pd.DataFrame(nmf_35_topic.round(5), columns = topic_names)\n",
    "\n",
    "#merge dataframes\n",
    "df = pd.concat([df.reset_index(drop=True), H_35.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Aggregation of speeches py parliament member"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T04:20:28.530717Z",
     "start_time": "2019-09-24T04:20:28.527025Z"
    }
   },
   "source": [
    "## 7.1. Aggregation preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:27:35.232149Z",
     "start_time": "2019-10-14T23:27:35.164816Z"
    }
   },
   "outputs": [],
   "source": [
    "# get sum of topics per speech\n",
    "df['topic_sum']= 0\n",
    "for topic in topic_names:\n",
    "    df['topic_sum'] += df[topic]\n",
    "    \n",
    "#get percentage share of topic per speech\n",
    "aggregation_columns = topic_names + ['topic_sum']\n",
    "\n",
    "for topic in aggregation_columns:\n",
    "    df[topic] = df[topic]/df['topic_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:27:35.505438Z",
     "start_time": "2019-10-14T23:27:35.233994Z"
    }
   },
   "outputs": [],
   "source": [
    "#remove speeches of non-parliament members\n",
    "df_parties = df[df['party']!='Other']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Speech aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:27:35.818052Z",
     "start_time": "2019-10-14T23:27:35.507700Z"
    }
   },
   "outputs": [],
   "source": [
    "#sum word count and topic columns for each parliament member\n",
    "sum_columns = ['word_count'] + aggregation_columns\n",
    "\n",
    "df_speaker_parties = df_parties.groupby(['full_name','party'])[sum_columns].apply(lambda x: x.sum()).reset_index()\n",
    "\n",
    "#get percentage share of topic per parliament members\n",
    "for topic in topic_names:\n",
    "    df_speaker_parties[topic] = df_speaker_parties[topic]/df_speaker_parties['topic_sum']\n",
    "    \n",
    "df_speaker_parties.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T04:36:38.899631Z",
     "start_time": "2019-09-24T04:36:38.896271Z"
    }
   },
   "source": [
    "# 8. Cluster analysis (k=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:27:35.969237Z",
     "start_time": "2019-10-14T23:27:35.820226Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=15, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=44, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run cluster analysis\n",
    "km_15 = KMeans(n_clusters=15,random_state=44)\n",
    "\n",
    "X_speaker = df_speaker_parties[topic_names].values\n",
    "km_15.fit(X_speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:27:36.060585Z",
     "start_time": "2019-10-14T23:27:36.053637Z"
    }
   },
   "outputs": [],
   "source": [
    "#name cluster based on topic distribution in centers\n",
    "cluster_dict = {0:'Sicherheitspolitik', 1:'Umwelt-/Energiepolitik',2:'Europapolitik',3:'Gesundheitspolitik',\n",
    "                4:'Arbeitsmarktpolitik',5:'Wirtschafts-/Finanzpolitik', 6:'Digitalisierung',7:'Demokratie/Rechtstaatlichkeit',\n",
    "                8:'Innen-/Justizpolitik',9:'Verkehr/Infrastruktur',10:'Familienpolitik', 11:'Außenpolitik',12:'Pflegepolitik',\n",
    "                13:'Bildungspolitik',14:'Landwirtschaftspolitik'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T23:27:36.089429Z",
     "start_time": "2019-10-14T23:27:36.072583Z"
    }
   },
   "outputs": [],
   "source": [
    "#add clusters to dataframe\n",
    "df_speaker_parties['cluster'] = km_15.labels_\n",
    "df_speaker_parties['cluster_name'] = df_speaker_parties['cluster'].map(cluster_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
